{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 EEG files.\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/303A_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "298 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/301C_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "284 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/305C_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "296 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/302B_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/302A_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "297 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/303B_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "298 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/304C_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/302C_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/305B_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "290 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/301B_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/304A_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "289 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/304B_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "296 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/303C_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "299 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/305A_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "296 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /Users/rasmusarnmark/Desktop/Sem4/fagprojekt/fagprojekt/data/301A_FG_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    5498.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "300 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Final EEG Tensor Shape: torch.Size([2674, 64, 3000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mne\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Load metadata DataFrame\n",
    "df_info = pd.read_pickle(\"data/FG_overview_df_v2.pkl\")  # Update with actual path\n",
    "\n",
    "# Define event IDs\n",
    "event_labels = {'T1P': 301, 'T1Pn': 302, 'T3P': 303, 'T3Pn': 304,\n",
    "                'T12P': 305, 'T12Pn': 306, 'T13P': 307, 'T13Pn': 308,\n",
    "                'T23P': 309, 'T23Pn': 310}\n",
    "\n",
    "file_paths = glob.glob(\"data/*_FG_preprocessed-epo.fif\")  # Update with actual data path\n",
    "print(f\"Found {len(file_paths)} EEG files.\")\n",
    "\n",
    "all_eeg_data, all_labels = [], []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract filename (e.g., \"301A\")\n",
    "    file_name = file_path.split(\"/\")[-1].split(\"_\")[0]  # Extract \"301A\"\n",
    "\n",
    "    # Extract Experiment ID (e.g., \"301\")\n",
    "    exp_id = file_name[:4]\n",
    "\n",
    "    # Get participants for this experiment\n",
    "    experiment_participants = df_info[df_info[\"Exp_id\"] == exp_id]\n",
    "\n",
    "    if experiment_participants.empty:\n",
    "        print(f\"Skipping {file_name}: No participants found for {exp_id}.\")\n",
    "        continue\n",
    "\n",
    "    # Load EEG file\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    eeg_data = epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "    labels = epochs.events[:, -1]  # Extract event labels\n",
    "\n",
    "    # Process each participant in the experiment\n",
    "    for _, row in experiment_participants.iterrows():\n",
    "        subject_id = row[\"Subject_id\"]\n",
    "        eeg_device = row[\"EEG_device\"]  # 1, 2, or 3\n",
    "\n",
    "        # Map EEG device to the corresponding event labels\n",
    "        device_event_labels = {\n",
    "            1: {301, 302, 305, 306, 307, 308},  # T1 labels\n",
    "            2: {303, 304, 305, 306, 309, 310},  # T2 labels\n",
    "            3: {307, 308, 309, 310, 303, 304},  # T3 labels\n",
    "        }\n",
    "\n",
    "        valid_events = device_event_labels[eeg_device]\n",
    "\n",
    "        # Filter trials for this subject\n",
    "        valid_trials = [i for i, label in enumerate(labels) if label in valid_events]\n",
    "\n",
    "        if len(valid_trials) == 0:\n",
    "            print(f\"Skipping subject {subject_id} in {exp_id}: No relevant trials for EEG device {eeg_device}.\")\n",
    "            continue\n",
    "\n",
    "        # Keep only the relevant trials\n",
    "        eeg_subject_data = eeg_data[valid_trials]\n",
    "        labels_subject = labels[valid_trials]\n",
    "\n",
    "        # Normalize per file\n",
    "        eeg_subject_data = (eeg_subject_data - eeg_subject_data.mean()) / eeg_subject_data.std()\n",
    "\n",
    "        # Convert labels to binary classification (feedback vs. no feedback)\n",
    "        binary_labels = np.array([1 if label in {301, 303, 305, 307, 309} else 0 for label in labels_subject])\n",
    "\n",
    "        all_eeg_data.append(eeg_subject_data)\n",
    "        all_labels.append(binary_labels)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "eeg_tensor = torch.tensor(np.concatenate(all_eeg_data, axis=0), dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(np.concatenate(all_labels, axis=0), dtype=torch.long)\n",
    "\n",
    "print(f\"Final EEG Tensor Shape: {eeg_tensor.shape}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180,)\n",
      "(175,)\n",
      "(177,)\n",
      "(180,)\n",
      "(180,)\n",
      "(179,)\n",
      "(180,)\n",
      "(180,)\n",
      "(176,)\n",
      "(180,)\n",
      "(173,)\n",
      "(176,)\n",
      "(180,)\n",
      "(178,)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_labels)):\n",
    "    print(all_labels[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_id</th>\n",
       "      <th>Exp_id</th>\n",
       "      <th>Friend_status</th>\n",
       "      <th>EEG_device</th>\n",
       "      <th>Force_device</th>\n",
       "      <th>Force_port</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Class_friends</th>\n",
       "      <th>Class_close_friends</th>\n",
       "      <th>Friends</th>\n",
       "      <th>Close_friends</th>\n",
       "      <th>Triad_id</th>\n",
       "      <th>Participant</th>\n",
       "      <th>tFriends</th>\n",
       "      <th>tClose_friends</th>\n",
       "      <th>All_friends</th>\n",
       "      <th>tClass</th>\n",
       "      <th>rClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1049</td>\n",
       "      <td>301A</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.239562</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>301</td>\n",
       "      <td>P1</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1029</td>\n",
       "      <td>301B</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>24.331280</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>301</td>\n",
       "      <td>P2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028</td>\n",
       "      <td>301C</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21.670089</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>301</td>\n",
       "      <td>P3</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1064</td>\n",
       "      <td>302A</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.009582</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>302</td>\n",
       "      <td>P1</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024</td>\n",
       "      <td>302B</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21.355236</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>302</td>\n",
       "      <td>P2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6069</td>\n",
       "      <td>330B</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19.373032</td>\n",
       "      <td>F</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>330</td>\n",
       "      <td>P2</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6048</td>\n",
       "      <td>330C</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19.841205</td>\n",
       "      <td>M</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>330</td>\n",
       "      <td>P3</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>75</td>\n",
       "      <td>24</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>6015</td>\n",
       "      <td>331A</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.052019</td>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>331</td>\n",
       "      <td>P1</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>6183</td>\n",
       "      <td>331B</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>18.863792</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>331</td>\n",
       "      <td>P2</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6141</td>\n",
       "      <td>331C</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19.633128</td>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>331</td>\n",
       "      <td>P3</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject_id Exp_id Friend_status  EEG_device  Force_device  Force_port  \\\n",
       "0         1049   301A           Yes           1             1           1   \n",
       "1         1029   301B            No           2             3           2   \n",
       "2         1028   301C           Yes           3             4           3   \n",
       "3         1064   302A           Yes           1             1           1   \n",
       "4         1024   302B           Yes           2             3           2   \n",
       "..         ...    ...           ...         ...           ...         ...   \n",
       "87        6069   330B            No           2             3           2   \n",
       "88        6048   330C           Yes           3             4           3   \n",
       "89        6015   331A           Yes           1             1           1   \n",
       "90        6183   331B           Yes           2             3           2   \n",
       "91        6141   331C            No           3             4           3   \n",
       "\n",
       "          Age Gender  Class_friends  Class_close_friends  Friends  \\\n",
       "0   22.239562      F              3                    6        9   \n",
       "1   24.331280      M              3                    1        8   \n",
       "2   21.670089      M              8                    4        9   \n",
       "3   22.009582      M             14                    8       11   \n",
       "4   21.355236      M              5                    7        6   \n",
       "..        ...    ...            ...                  ...      ...   \n",
       "87  19.373032      F             14                    3       17   \n",
       "88  19.841205      M             13                   11       25   \n",
       "89  20.052019      M              9                    4       14   \n",
       "90  18.863792      M             22                    6       25   \n",
       "91  19.633128      M              9                    3       17   \n",
       "\n",
       "    Close_friends  Triad_id Participant  tFriends  tClose_friends  \\\n",
       "0              14       301          P1        12              20   \n",
       "1               5       301          P2        11               6   \n",
       "2               4       301          P3        17               8   \n",
       "3               5       302          P1        25              13   \n",
       "4              10       302          P2        11              17   \n",
       "..            ...       ...         ...       ...             ...   \n",
       "87             17       330          P2        31              20   \n",
       "88             26       330          P3        38              37   \n",
       "89             13       331          P1        23              17   \n",
       "90             12       331          P2        47              18   \n",
       "91             10       331          P3        26              13   \n",
       "\n",
       "    All_friends  tClass    rClass  \n",
       "0            32       9  0.666667  \n",
       "1            17       4  0.250000  \n",
       "2            25      12  0.333333  \n",
       "3            38      22  0.363636  \n",
       "4            28      12  0.583333  \n",
       "..          ...     ...       ...  \n",
       "87           51      17  0.176471  \n",
       "88           75      24  0.458333  \n",
       "89           40      13  0.307692  \n",
       "90           65      28  0.214286  \n",
       "91           39      12  0.250000  \n",
       "\n",
       "[92 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaBraM(\n",
       "  (patch_embed): TemporalConv(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(1, 15), stride=(1, 8), padding=(0, 7))\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (norm1): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (gelu2): GELU(approximate='none')\n",
       "    (norm2): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (conv3): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (norm3): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (gelu3): GELU(approximate='none')\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=200, out_features=600, bias=False)\n",
       "        (q_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (k_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=200, out_features=800, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=800, out_features=200, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=200, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torcheeg.models import LaBraM\n",
    "\n",
    "# Define electrode names to upper\n",
    "electrode_names = epochs.ch_names\n",
    "electrode_names = [name.upper() for name in electrode_names]\n",
    "\n",
    "model = LaBraM(num_electrodes=len(electrode_names), electrodes=electrode_names)\n",
    "\n",
    "# Load pre-trained weights (if available)\n",
    "# model.load_state_dict(torch.load(\"path_to_pretrained_labram.pth\"))\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     16\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43melectrode_names\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass electrodes explicitly\u001b[39;00m\n\u001b[32m     18\u001b[39m     loss = criterion(outputs, batch_y)\n\u001b[32m     19\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torcheeg/models/transformer/labram.py:448\u001b[39m, in \u001b[36mLaBraM.forward\u001b[39m\u001b[34m(self, x, electrodes, return_patch_tokens, return_all_tokens, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, electrodes=[], return_patch_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, return_all_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_patch_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_patch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_all_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.head(x)\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torcheeg/models/transformer/labram.py:405\u001b[39m, in \u001b[36mLaBraM.forward_features\u001b[39m\u001b[34m(self, x, electrodes, return_patch_tokens, return_all_tokens, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(standard_1020) == x.shape[\u001b[32m1\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou must provide electrodes for the input. Expected default channels \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard_1020\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m batch_size, n, a, t = x.shape\n\u001b[32m    406\u001b[39m input_time_window = a \u001b[38;5;28;01mif\u001b[39;00m t == \u001b[38;5;28mself\u001b[39m.patch_size \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[32m    407\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(eeg_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X, electrodes=electrode_names)  # Pass electrodes explicitly\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
