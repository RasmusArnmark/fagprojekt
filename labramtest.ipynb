{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import torch\n",
    "import numpy as np\n",
    "from torcheeg.models import LaBraM\n",
    "\n",
    "# Load a sample EEG file\n",
    "file_path = \"data/301A_FG_preprocessed-epo.fif\"  # Update with your actual file\n",
    "epochs = mne.read_epochs(file_path, preload=True)\n",
    "\n",
    "# Extract data and labels\n",
    "eeg_data = epochs.get_data()  # Shape: (n_epochs, n_channels, n_times)\n",
    "labels = epochs.events[:, -1]  # Assuming the last column contains event labels\n",
    "\n",
    "# Normalize EEG data\n",
    "eeg_data = (eeg_data - eeg_data.mean()) / eeg_data.std()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "eeg_tensor = torch.tensor(eeg_data, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Extract the actual electrode names\n",
    "electrode_names = [ch.upper() for ch in epochs.ch_names ] # List of channel names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FP1',\n",
       " 'AF7',\n",
       " 'AF3',\n",
       " 'F1',\n",
       " 'F3',\n",
       " 'F5',\n",
       " 'F7',\n",
       " 'FT7',\n",
       " 'FC5',\n",
       " 'FC3',\n",
       " 'FC1',\n",
       " 'C1',\n",
       " 'C3',\n",
       " 'C5',\n",
       " 'T7',\n",
       " 'TP7',\n",
       " 'CP5',\n",
       " 'CP3',\n",
       " 'CP1',\n",
       " 'P1',\n",
       " 'P3',\n",
       " 'P5',\n",
       " 'P7',\n",
       " 'P9',\n",
       " 'PO7',\n",
       " 'PO3',\n",
       " 'O1',\n",
       " 'IZ',\n",
       " 'OZ',\n",
       " 'POZ',\n",
       " 'PZ',\n",
       " 'CPZ',\n",
       " 'FPZ',\n",
       " 'FP2',\n",
       " 'AF8',\n",
       " 'AF4',\n",
       " 'AFZ',\n",
       " 'FZ',\n",
       " 'F2',\n",
       " 'F4',\n",
       " 'F6',\n",
       " 'F8',\n",
       " 'FT8',\n",
       " 'FC6',\n",
       " 'FC4',\n",
       " 'FC2',\n",
       " 'FCZ',\n",
       " 'CZ',\n",
       " 'C2',\n",
       " 'C4',\n",
       " 'C6',\n",
       " 'T8',\n",
       " 'TP8',\n",
       " 'CP6',\n",
       " 'CP4',\n",
       " 'CP2',\n",
       " 'P2',\n",
       " 'P4',\n",
       " 'P6',\n",
       " 'P8',\n",
       " 'P10',\n",
       " 'PO8',\n",
       " 'PO4',\n",
       " 'O2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electrode_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LaBraM(\n",
       "  (patch_embed): TemporalConv(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(1, 15), stride=(1, 8), padding=(0, 7))\n",
       "    (gelu1): GELU(approximate='none')\n",
       "    (norm1): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (gelu2): GELU(approximate='none')\n",
       "    (norm2): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (conv3): Conv2d(8, 8, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (norm3): GroupNorm(4, 8, eps=1e-05, affine=True)\n",
       "    (gelu3): GELU(approximate='none')\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=200, out_features=600, bias=False)\n",
       "        (q_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (k_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=200, out_features=800, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=800, out_features=200, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=200, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LaBraM(num_electrodes=len(electrode_names), electrodes=electrode_names)\n",
    "\n",
    "# Load pre-trained weights (if available)\n",
    "# model.load_state_dict(torch.load(\"path_to_pretrained_labram.pth\"))\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     17\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43melectrode_names\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass electrodes explicitly\u001b[39;00m\n\u001b[32m     19\u001b[39m     loss = criterion(outputs, batch_y)\n\u001b[32m     20\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torcheeg/models/transformer/labram.py:448\u001b[39m, in \u001b[36mLaBraM.forward\u001b[39m\u001b[34m(self, x, electrodes, return_patch_tokens, return_all_tokens, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, electrodes=[], return_patch_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, return_all_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43melectrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_patch_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_patch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_all_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.head(x)\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fp/lib/python3.11/site-packages/torcheeg/models/transformer/labram.py:405\u001b[39m, in \u001b[36mLaBraM.forward_features\u001b[39m\u001b[34m(self, x, electrodes, return_patch_tokens, return_all_tokens, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(standard_1020) == x.shape[\u001b[32m1\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou must provide electrodes for the input. Expected default channels \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard_1020\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m batch_size, n, a, t = x.shape\n\u001b[32m    406\u001b[39m input_time_window = a \u001b[38;5;28;01mif\u001b[39;00m t == \u001b[38;5;28mself\u001b[39m.patch_size \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[32m    407\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(eeg_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X, electrodes=electrode_names)  # Pass electrodes explicitly\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
